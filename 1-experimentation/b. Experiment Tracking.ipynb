{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging Things Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "# parameter\n",
    "\n",
    "mlflow.log_param('name', 'value')\n",
    "mlflow.log_params({'name1': 'value1'})\n",
    "mlflow.log_metric('metric1', 1)\n",
    "mlflow.log_metrics('Dictionary of metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging a model manually\n",
    "# from https://mlflow.org/docs/latest/getting-started/intro-quickstart/index.html\n",
    "# Set our tracking server uri for logging\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "\n",
    "# Create a new MLflow Experiment\n",
    "mlflow.set_experiment(\"MLflow Quickstart\")\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run() as run:\n",
    "    run_id = run.run_info.run_id\n",
    "    # Log the hyperparameters\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    # Log the loss metric\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    # Set a tag that we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"Training Info\", \"Basic LR model for iris data\")\n",
    "\n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(X_train, lr.predict(X_train))\n",
    "\n",
    "    # Log the model\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=lr,\n",
    "        artifact_path=\"iris_model\",\n",
    "        signature=signature,\n",
    "        input_example=X_train,\n",
    "        registered_model_name=\"tracking-quickstart\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata\n",
    "All MLflow runs are logged to the active experiment, which can be set using any of the following ways:\n",
    "\n",
    "Use the mlflow.set_experiment() command.\n",
    "\n",
    "Use the experiment_id parameter in the mlflow.start_run() command.\n",
    "\n",
    "Set one of the MLflow environment variables MLFLOW_EXPERIMENT_NAME or MLFLOW_EXPERIMENT_ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searching runs\n",
    "mlflow.search_runs(experiment_ids: Optional[List[str]] = None, filter_string: str = '', run_view_type: int = 1, max_results: int = 100000, order_by: Optional[List[str]] = None, output_format: str = 'pandas', search_all_experiments: bool = False, experiment_names: Optional[List[str]] = None) → Union[List[Run], pandas.DataFrame]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating existing model signature\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import mlflow\n",
    "from mlflow.models.model import get_model_info\n",
    "from mlflow.models import infer_signature, set_signature\n",
    "\n",
    "# load the logged model\n",
    "model_uri = f\"runs:/{run.info.run_id}/iris_rf\"\n",
    "model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# construct the model signature from test dataset\n",
    "X_test, _ = datasets.load_iris(return_X_y=True, as_frame=True)\n",
    "signature = infer_signature(X_test, model.predict(X_test))\n",
    "\n",
    "# set the signature for the logged model\n",
    "set_signature(model_uri, signature)\n",
    "\n",
    "# now when you load the model again, it will have the desired signature\n",
    "assert get_model_info(model_uri).signature == signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using predict parameters as part of the model signature\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from xgboost import XGBClassifier \n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(random_state=42)\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X, y)\n",
    "\n",
    "xgb.predict(X, iteration_range=(1, 3)) # iteration range\n",
    "\n",
    "signature = infer_signature(X, params={'iteration_range': [2, 5]})\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "  run_id = run.info.run_id\n",
    "  model_info = mlflow.sklearn.log_model(\n",
    "    sk_model=xgb, \n",
    "    artifact_path='xgb_model',\n",
    "    signature=signature\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = mlflow.sklearn.load_model(model_info.model_uri)\n",
    "loaded_model.predict(X, iteration_range=(10, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Input Example\n",
    "\n",
    "Including an input example while logging a model offers dual benefits. Firstly, it aids in inferring the model’s signature. Secondly, and just as importantly, it validates the model’s requirements. This input example is utilized to execute a prediction using the model that is about to be logged, thereby enhancing the accuracy in identifying model requirement dependencies. It is highly recommended to always include an input example along with your models when you log th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example code, called with logging the model\n",
    "input_example = {\n",
    "    \"sepal length (cm)\": 5.1,\n",
    "    \"sepal width (cm)\": 3.5,\n",
    "    \"petal length (cm)\": 1.4,\n",
    "    \"petal width (cm)\": 0.2,\n",
    "}\n",
    "mlflow.sklearn.log_model(..., input_example=input_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "disect https://mlflow.org/docs/latest/python_api/mlflow.shap.html "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
