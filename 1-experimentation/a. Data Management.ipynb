{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/31 16:05:51 WARN Utils: Your hostname, debian resolves to a loopback address: 127.0.1.1; using 192.168.100.203 instead (on interface wlp5s0)\n",
      "24/03/31 16:05:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/aadi/miniconda3/envs/pyspark_env/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/aadi/.ivy2/cache\n",
      "The jars for the packages stored in: /home/aadi/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3beb7ccb-4799-4454-b4bd-4fb2b4f044cc;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.2.0 in central\n",
      "\tfound io.delta#delta-storage;2.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.2.0/delta-core_2.12-2.2.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-core_2.12;2.2.0!delta-core_2.12.jar (872ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/2.2.0/delta-storage-2.2.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;2.2.0!delta-storage.jar (149ms)\n",
      ":: resolution report :: resolve 993ms :: artifacts dl 1025ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   2   |   2   |   0   ||   3   |   2   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3beb7ccb-4799-4454-b4bd-4fb2b4f044cc\n",
      "\tconfs: [default]\n",
      "\t2 artifacts copied, 1 already retrieved (3398kB/7ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/31 16:05:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/31 16:05:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"trying_delta\")\\\n",
    "  .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.2.0\")\\\n",
    "  .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\\\n",
    "  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Write a Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "df = DataFrame({'a': [1, 2, 3], 'val': [1, 2, 3]})\n",
    "df.to_csv('./test_csv.csv', index=False)\n",
    "\n",
    "df = spark.read.csv('./test_csv.csv', header=True)\n",
    "df.write.save('./test_csv.delta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a delta table\n",
    "PATH_TO_DELTA = './test_csv.delta/'\n",
    "delta_table = spark.read.load(PATH_TO_DELTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/31 16:14:21 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "+------+--------------------+----------------+-----------+--------------------+--------------------+--------------------+----------------+--------+-----------+----------+----------------+----------------+\n",
      "|format|                  id|            name|description|            location|           createdAt|        lastModified|partitionColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion|\n",
      "+------+--------------------+----------------+-----------+--------------------+--------------------+--------------------+----------------+--------+-----------+----------+----------------+----------------+\n",
      "| delta|b8794435-fdb3-4bb...|default.test_csv|       null|file:/storage/pro...|2024-03-31 16:14:...|2024-03-31 16:14:...|              []|       1|        656|        {}|               1|               2|\n",
      "+------+--------------------+----------------+-----------+--------------------+--------------------+--------------------+----------------+--------+-----------+----------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get location of managed table\n",
    "delta_table.write.format('delta').saveAsTable('test_csv')\n",
    "spark.sql('DESCRIBE DETAIL test_csv').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Delta Table History and Load Previous Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------\n",
      " version             | 0                    \n",
      " timestamp           | 2024-03-31 16:14:... \n",
      " userId              | null                 \n",
      " userName            | null                 \n",
      " operation           | CREATE TABLE AS S... \n",
      " operationParameters | {isManaged -> tru... \n",
      " job                 | null                 \n",
      " notebook            | null                 \n",
      " clusterId           | null                 \n",
      " readVersion         | null                 \n",
      " isolationLevel      | Serializable         \n",
      " isBlindAppend       | true                 \n",
      " operationMetrics    | {numFiles -> 1, n... \n",
      " userMetadata        | null                 \n",
      " engineInfo          | Apache-Spark/3.3.... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('DESCRIBE HISTORY default.test_csv').show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|val|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "|  2|  2|\n",
      "|  3|  3|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.option(\"timestampAsOf\", \"2019-01-01\").table(\"people10m\") # using date\n",
    "df2 = spark.read.option(\"versionAsOf\", 123).load(\"/tmp/delta/people10m\") # using numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "restoring a table \n",
    "```sql\n",
    "RESTORE TABLE db.target_table TO VERSION AS OF <version>\n",
    "RESTORE TABLE delta.`/data/target/` TO TIMESTAMP AS OF <timestamp>\n",
    "``````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create, Overwrite, Merge and read Feature Store Tables in Machine Learning Workflows\n",
    "\n",
    "Links: \n",
    "https://docs.databricks.com/en/machine-learning/feature-store/workspace-feature-store/feature-tables.html  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.feature_store import feature_table\n",
    "\n",
    "def compute_customer_features(data):\n",
    "  ''' Feature computation code returns a DataFrame with 'customer_id' as primary key'''\n",
    "  pass\n",
    "\n",
    "# create feature table keyed by customer_id\n",
    "# take schema from DataFrame output by compute_customer_features\n",
    "from databricks.feature_store import FeatureStoreClient\n",
    "\n",
    "customer_features_df = compute_customer_features(df)\n",
    "\n",
    "fs = FeatureStoreClient()\n",
    "\n",
    "customer_feature_table = fs.create_table(\n",
    "  name='recommender_system.customer_features',\n",
    "  primary_keys='customer_id',\n",
    "  schema=customer_features_df.schema,\n",
    "  description='Customer features'\n",
    ")\n",
    "\n",
    "# An alternative is to use `create_table` and specify the `df` argument.\n",
    "# This code automatically saves the features to the underlying Delta table.\n",
    "\n",
    "# customer_feature_table = fs.create_table(\n",
    "#  ...\n",
    "#  df=customer_features_df,\n",
    "#  ...\n",
    "# )\n",
    "\n",
    "# To use a composite key, pass all keys in the create_table call\n",
    "\n",
    "# customer_feature_table = fs.create_table(\n",
    "#   ...\n",
    "#   primary_keys=['customer_id', 'date'],\n",
    "#   ...\n",
    "# )\n",
    "\n",
    "# Use write_table to write data to the feature table\n",
    "# Overwrite mode does a full refresh of the feature table\n",
    "\n",
    "fs.write_table(\n",
    "  name='recommender_system.customer_features',\n",
    "  df = customer_features_df,\n",
    "  mode = 'overwrite'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register Existing Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.register_table(\n",
    "  delta_table='database.table_name',\n",
    "  primary_keys=['id'],\n",
    "  description='something meaningful'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add New Features\n",
    "\n",
    "Basically just rerun `write_table` with the new features, this also works to update only specific rows, where the primary key doesn't exist, the rows do not get edited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.feature_store import feature_table\n",
    "\n",
    "def compute_customer_features_2(data):\n",
    "  ''' Feature computation code returns a DataFrame with 'customer_id' as primary key'''\n",
    "  pass\n",
    "\n",
    "# create feature table keyed by customer_id\n",
    "# take schema from DataFrame output by compute_customer_features\n",
    "from databricks.feature_store import FeatureStoreClient\n",
    "\n",
    "customer_features_df = compute_customer_features_2(df)\n",
    "\n",
    "fs = FeatureStoreClient()\n",
    "\n",
    "fs.write_table(\n",
    "  name='recommender_system.customer_features',\n",
    "  df = customer_features_df,\n",
    "  mode = 'merge'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read from a Feature Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = feature_store.FeatureStoreClient()\n",
    "customer_features_df = fs.read_table(\n",
    "  name='recommender.customer_features',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n",
    "1. To add tags, use the `tags` parameter of type `dict` when calling `create_table`\n",
    "2. Use `feature_store_client_instance.set_feature_table_tag` or `.delete_feature_table_tag` for existing tables  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Data Sources "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.feature_store import FeatureStoreClient\n",
    "fs = FeatureStoreClient()\n",
    "\n",
    "# Use `source_type=\"table\"` to add a table in the metastore as data source.\n",
    "fs.add_data_sources(feature_table_name=\"clicks\", data_sources=\"user_info.clicks\", source_type=\"table\")\n",
    "\n",
    "# Use `source_type=\"path\"` to add a data source in path format.\n",
    "fs.add_data_sources(feature_table_name=\"user_metrics\", data_sources=\"dbfs:/FileStore/user_metrics.json\", source_type=\"path\")\n",
    "\n",
    "# Use `source_type=\"custom\"` if the source is not a table or a path.\n",
    "fs.add_data_sources(feature_table_name=\"user_metrics\", data_sources=\"user_metrics.txt\", source_type=\"custom\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleta a Feature Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when calling this, the underlying delta table is also dropped \n",
    "\n",
    "fs.drop_table(\n",
    "  name='recommender_system.customer_features'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
